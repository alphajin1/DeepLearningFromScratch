# 이번 장에서 배운 내용
* Embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어 ID의 벡터를 추출.
* word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋다.
* negative sampling 은 부정적 예를 몇 개 샘플링하는 기법으로, 이를 이용하면 다중분류를 이진분류처럼 취급할 수 있다.
* word2vec으로 얻은 단어 임베딩에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.
* word2vec의 단어의 임베딩을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.
* word2vec는 전이 학습(transfer learning)측면에서 특히 중요하고, 그 단어의 임베딩은 다양한 자연어 처리 작업에 이용할 수 있다.

## 기타
* 없음